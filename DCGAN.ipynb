{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "\n",
    "### ToDo\n",
    "* DCGAN\n",
    "    * MNIST 에 맞게 구현하였는데 실제로 논문에서는 어떻게?\n",
    "    * keras MNIST 데이터셋 체크\n",
    "    * Adam-a/b, init 등 다른 파라메터 체크\n",
    "* Tensorboard with summary\n",
    "* 모델이 잘 작동하는지 확인해보고싶다.\n",
    "    * 특히, BN 의 모멘텀을 확인해 볼 수 있나?\n",
    "* Other datasets\n",
    "    * Keras 에서 다른 데이터셋도 지원하는듯?\n",
    "* Refactoring with model.py (ipynb 에 몰빵한걸 분리)\n",
    "\n",
    "### Default\n",
    "* 기본적으로 이미지를 [-1, 1] 로 normalize\n",
    "\n",
    "### Discriminator\n",
    "![discriminator](http://bamos.github.io/data/2016-08-09/discrim-architecture.png)\n",
    "\n",
    "* Activation function: Leaky ReLU\n",
    "* BN 은 첫번째 layer 에는 붙지 않음\n",
    "    * Generator 가 생성한 sample 의 statistics 를 잘 학습하기 위함이라고 함\n",
    "\n",
    "### Generator\n",
    "![generator](http://bamos.github.io/data/2016-08-09/gen-architecture.png)\n",
    "\n",
    "* Activation function: ReLU / 마지막 레이어만 tanh\n",
    "    * 기본적으로 이미지를 [-1, 1] 로 normalize 하기때문에 tanh results 가 바로 output 이 됨\n",
    "* z_dim 은 기본적으로는 100인듯. 첫번째에서는 FC 로 차원수를 알맞게 늘린 후 reshape.\n",
    "* BN 은 output layer 에는 붙지 않음 & FC layer 로 project 할때도 붙지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis=('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2):\n",
    "    return tf.maximum(x, x*leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False):\n",
    "    # x: [-1, 28, 28, 1]\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        with tf.variable_scope(\"D_conv1\"):\n",
    "            h1_conv = tf.layers.conv2d(inputs=x, filters=64, kernel_size=[5, 5], strides=2, \n",
    "                                       padding='SAME', use_bias=True, reuse=reuse)\n",
    "            h1 = lrelu(h1_conv) # [-1, 14, 14, 64]\n",
    "        with tf.variable_scope(\"D_conv2\"):\n",
    "            h2_conv = tf.layers.conv2d(inputs=h1, filters=128, kernel_size=[5, 5], strides=2, \n",
    "                                       padding='SAME', use_bias=False, reuse=reuse)\n",
    "            h2_bn = tf.layers.batch_normalization(h2_conv, training=training, reuse=reuse)\n",
    "            h2 = lrelu(h2_bn) # [-1, 7, 7, 128]\n",
    "        with tf.variable_scope(\"D_dense\"):\n",
    "            logits = tf.layers.dense(inputs=h2, units=1, name=\"D_logits\", reuse=reuse)\n",
    "            prob = tf.sigmoid(logits, \"D_prob\")\n",
    "        \n",
    "        return prob, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    # z: [-1, 100]\n",
    "    # 128 -> 64 -> output\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        with tf.variable_scope(\"G_project_reshape\"):\n",
    "            h1_dense = tf.layers.dense(z, 128*7*7, activation=tf.nn.relu)\n",
    "            h1 = tf.reshape(h1_dense, [-1, 7, 7, 128])\n",
    "        with tf.variable_scope(\"G_conv_T1\"): \n",
    "            h2_conv_T = tf.layers.conv2d_transpose(h1, 64, [5,5], strides=2, padding='SAME', use_bias=False)\n",
    "            h2_bn = tf.layers.batch_normalization(h2_conv_T, training=training)\n",
    "            h2 = tf.nn.relu(h2_bn) # [-1, 14, 14, 64]\n",
    "        with tf.variable_scope(\"G_conv_T2\"): \n",
    "            h3_conv_T = tf.layers.conv2d_transpose(h2, 1, [5,5], strides=2, padding='SAME', use_bias=False)\n",
    "#             h3_bn = tf.layers.batch_normalization(h3_conv_T, training=training)\n",
    "#             h3 = tf.nn.tanh(h3_conv_T) # [-1, 28, 28, 1]\n",
    "            h3 = tf.nn.sigmoid(h3_conv_T)\n",
    "            # mnist dataset 은 0~1 이므로, tanh 에 맞춰주기 위해 초기 normalization 을 해주기로 하자.\n",
    "            # 라고 생각했지만 귀찮아서 그냥 시그모이드로 해주기로 하자.\n",
    "            \n",
    "        return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Z_sampler(m, n):\n",
    "    # 가우시안이 더 좋다고 함\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "#     return tf.random_uniform([m, n], minval=-1., maxval=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784], name='X')\n",
    "Z = tf.placeholder(tf.float32, [None, 100], name='Z')\n",
    "training = tf.placeholder(tf.bool, name='training')\n",
    "\n",
    "# calc loss\n",
    "G_sample = generator(Z)\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "D_prob_real, D_logit_real = discriminator(X_img)\n",
    "D_prob_fake, D_logit_fake = discriminator(G_sample, reuse=True)\n",
    "\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, \n",
    "                                                                     labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake,\n",
    "                                                                     labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, \n",
    "                                                                labels=tf.ones_like(D_logit_fake)))\n",
    "# solvers\n",
    "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "\n",
    "# TF 에서 batchnorm 을 쓰면 moving_mean / moving_variance 를 게산해야 함. \n",
    "# 이 두 변수를 업데이트하는 모멘텀 오퍼레이션이 GraphKeys.UPDATE_OPS 로 잡혀있대.\n",
    "# 그래서 그냥 돌리면 학습과정에서 모멘텀 오퍼레이션이 작동을 안해서 업데이트가 안 됨. \n",
    "# 따라서, control_dependencies 를 사용해서 업데이트하도록 지정해 주어야 한다.\n",
    "# 아래와 같이 control_dependencies 를 사용하면 D_solver 오퍼레이션이 작동하기 전에 D_update_ops 오퍼레이션이 먼저 작동하도록 지정해주는 것.\n",
    "# https://www.facebook.com/groups/TensorFlowKR/permalink/447827285558335/\n",
    "D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='discriminator')\n",
    "G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='generator')\n",
    "lr = 0.0002\n",
    "\n",
    "with tf.control_dependencies(D_update_ops):\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=lr).minimize(D_loss, var_list=D_vars)\n",
    "with tf.control_dependencies(G_update_ops):\n",
    "    G_solver = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss, var_list=G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist.validation.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_n = 500\n",
    "batch_size = 64\n",
    "N = mnist.train.num_examples\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    for _ in range(N // batch_size):\n",
    "        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # 'Z' 가 겹쳐도 상관 없을것 같다.\n",
    "        feeds = {X: X_batch, Z: Z_sampler(batch_size, 100), training: True}\n",
    "        _ = sess.run(D_solver, feeds)\n",
    "        _ = sess.run(G_solver, feeds)\n",
    "    \n",
    "    # progress check\n",
    "    # 1. loss check\n",
    "    feeds[training] = False\n",
    "    D_loss_cur = sess.run(D_loss, feeds)\n",
    "    G_loss_cur = sess.run(G_loss, feeds)\n",
    "    print(\"[{}/{}] D_loss: {:.4f} / G_loss: {:.4f}\".format(epoch+1, epoch_n, D_loss_cur, G_loss_cur))\n",
    "    \n",
    "    # 2. sampling\n",
    "    samples = sess.run(G_sample, {Z: Z_sampler(16, 100), training: False})\n",
    "    fig = plot(samples)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
